{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single atom training\n",
    "This notebook goes through the workflow of setting the hyperparameters, collecting atom manipulation data, and training the deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "from AMRL import RealExpEnv, Episode_Memory, Createc_Controller, sac_agent, ReplayMemory, HerReplayMemory\n",
    "from AMRL import plot_graph, show_reset, show_done, show_step\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the anchor image\n",
    "This cell retrieves the current STM scan image and use it as the template for positioning the anchor in STM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createc_controller = Createc_Controller(None, None, None, None)\n",
    "img_forward = np.array(createc_controller.stm.scandata(1,4))\n",
    "#TODO\n",
    "#Set the pixel of the top-left corner, widht, and height of the anchor\n",
    "#If the anchor is not used, just set w and h to a small number like below\n",
    "top_left, w, h = (0,0), 3, 3\n",
    "template = img_forward[top_left[1]:top_left[1]+h, top_left[0]:top_left[0]+w]\n",
    "plt.imshow(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters and create a RealExpEnv object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "step_nm = 0.4 #Set the radius of the manipulation start position relative the the atom start position\n",
    "goal_nm  = 2 #Set the radius of the manipulation end position relative the the atom start position\n",
    "max_mvolt = 15 #Set the maximum bias voltage in mV \n",
    "max_pcurrent_to_mvolt_ratio = 6E3 #Set the maximum conductance in pA/mV\n",
    "max_len = 5 #Set maximum episode length\n",
    "template_max_y = 3 #Set the maximum or minimum row number to search for anchor\n",
    "#Set the path to load CNN weight for the atom movement classifier\n",
    "CNN_weight_path = 'C:/LocalUserData/User-data/phys-asp-lab/auto_manipulation/training_Ag_retrain_2/_atom_move_detector_conv_2640.pth'\n",
    "current_jump  = 4 #Set the current jump gradient/ std(current) threshold required to take STM scan\n",
    "\n",
    "#Set STM scan parameters\n",
    "pixel = 128 \n",
    "im_size_nm = 7 #Image size in nm \n",
    "scan_mV = 500 #bias voltage\n",
    "x_nm, y_nm = createc_controller.get_offset_nm()\n",
    "offset_nm = np.array([x_nm, y_nm]) #Set offset to current offset value\n",
    "\n",
    "#Set manipulation parameters to pull atoms from image edge to center\n",
    "pull_back_mV = 5 #bias in mV\n",
    "pull_back_pA = 60000 #current in pA\n",
    "\n",
    "#Set manipulation limit [left, right, up, down] in nm\n",
    "manip_limit_nm = np.array([x_nm - 0.5*im_size_nm+0.25, x_nm + 0.5*im_size_nm-0.25, y_nm+0.25, y_nm+im_size_nm-0.25])\n",
    "\n",
    "env = RealExpEnv(step_nm, max_mvolt, max_pcurrent_to_mvolt_ratio, goal_nm, \n",
    "                 template, current_jump, im_size_nm, offset_nm, manip_limit_nm, pixel, \n",
    "                 template_max_y, scan_mV, max_len, \n",
    "                 CNN_weight_path, \n",
    "                 bottom=False, random_scan_rate = 0.8, pull_back_mV = pull_back_mV,\n",
    "                 pull_back_pA = pull_back_pA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sac_agent object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "batch_size= 64 #Set minibatch size\n",
    "LEARNING_RATE = 0.0003 #Set learning rate\n",
    "\n",
    "#Set the action space range\n",
    "ACTION_SPACE = namedtuple('ACTION_SPACE', ['high', 'low'])\n",
    "action_space = ACTION_SPACE(high = torch.tensor([1,1,1,1,1,1]), low = torch.tensor([-1,-1,-1,-1,1/3,1/2]))\n",
    "\n",
    "#Initialize the soft actor-critic agent\n",
    "agent = sac_agent(num_inputs = 4, num_actions = 6, action_space = action_space, device=device, hidden_size=256, lr=LEARNING_RATE,\n",
    "                 gamma=0.9, tau=0.005, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a HerReplayMemory object\n",
    "Here we use the hindsight experience replay with the 'future' strategy to sample goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "replay_size=1000000 #Set memory size\n",
    "\n",
    "memory = HerReplayMemory(replay_size, env, strategy = 'future')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Episode_Memory object\n",
    "The episode memory class is used to store all the relavant information in each training episode, including the STM images, state, action, reward, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_memory = Episode_Memory()\n",
    "#TODO\n",
    "#Set the folder name to store training data and neural network weight\n",
    "folder_name = 'C:/LocalUserData/User-data/phys-asp-lab/auto_manipulation/training_Ag_retrain_2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters for Emphasize Recent Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_k_min = 500\n",
    "eta = 0.994\n",
    "max_ep_len = max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty lists for logging performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards, alphas, precisions, episode_lengths = [], [], [], []\n",
    "avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_train(max_steps = max_len, num_episodes = 50, episode_start = 0):\n",
    "    \"\"\"\n",
    "    Collect training data and train the RL agent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_steps: int\n",
    "            maximum steps in an episode\n",
    "            \n",
    "    num_episodes: int\n",
    "            Train for this many episodes\n",
    "    \n",
    "    episode_start: int\n",
    "            Index to use for the starting episode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : None\n",
    "    \"\"\"\n",
    "    for i_episode in range(episode_start,episode_start+num_episodes):\n",
    "        print('Episode:', i_episode)\n",
    "        episode_reward, episode_steps = 0, 0\n",
    "        done = False\n",
    "        state, info = env.reset(update_conv_net=False)\n",
    "        show_reset(env.img_info, env.atom_start_absolute_nm, env.destination_absolute_nm,\n",
    "                   env.template_nm, env.template_wh)\n",
    "        episode_memory.update_memory_reset(env.img_info, i_episode, info)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            old_atom_nm = env.atom_absolute_nm\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_steps+=1\n",
    "            episode_reward+=reward\n",
    "            mask = float(not done)\n",
    "            memory.push(state,action,reward,next_state,mask)\n",
    "            episode_memory.update_memory_step(state, action, next_state, reward, done, info)\n",
    "            show_step(env.img_info, info['start_nm']+old_atom_nm, info['end_nm']+old_atom_nm,\n",
    "                        env.atom_absolute_nm, env.atom_start_absolute_nm, \n",
    "                        env.destination_absolute_nm, action[4]*env.max_mvolt, \n",
    "                        action[5]*env.max_pcurrent_to_mvolt_ratio*action[4]*env.max_mvolt, \n",
    "                        env.template_nm, env.template_wh)\n",
    "            print('step:', step,'reward', reward, 'precision:', env.dist_destination)\n",
    "            if done:\n",
    "                episode_memory.update_memory_done(env.img_info, env.atom_absolute_nm, env.atom_relative_nm)\n",
    "                episode_memory.save_memory(folder_name)\n",
    "                print('Episode reward:', episode_reward)\n",
    "                break\n",
    "            else:                \n",
    "                state=next_state\n",
    "             \n",
    "        if (len(memory)>batch_size):\n",
    "            episode_K = int(episode_steps)\n",
    "            for k in range(episode_K):\n",
    "                c_k = max(int(memory.__len__()*eta**((k)*(1000/episode_K))), 500)\n",
    "                agent.update_parameters(memory, batch_size, c_k)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        alphas.append(agent.alpha.item())\n",
    "        precisions.append(env.dist_destination)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        avg_episode_rewards.append(np.mean(episode_rewards[-min(100,len(episode_rewards)):]))\n",
    "        avg_alphas.append(np.mean(alphas[-min(100, len(alphas)):]))\n",
    "        avg_precisions.append(np.mean(precisions[-min(100, len(precisions)):]))\n",
    "        avg_episode_lengths.append(np.mean(episode_lengths[-min(100, len(episode_lengths)):]))\n",
    "        \n",
    "        if (i_episode+1)%2==0:\n",
    "            plot_graph(episode_rewards, precisions, alphas, episode_lengths,\n",
    "                      avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths)\n",
    "            \n",
    "        if (i_episode)%20 == 0:\n",
    "            torch.save(agent.critic.state_dict(), '{}/_critic_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.policy.state_dict(), '{}/_policy_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.alpha, '{}/_alpha_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(env.atom_move_detector.conv.state_dict(), '{}/_atom_move_detector_conv_{}.pth'.format(folder_name,i_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sac_train(episode_start = 2896,num_episodes = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
