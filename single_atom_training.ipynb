{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single atom training\n",
    "This notebook goes through the workflow of setting the hyperparameters, collecting atom manipulation data, and training the deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "from Environment.Env_new import RealExpEnv\n",
    "from RL.sac import sac_agent, ReplayMemory, HerReplayMemory\n",
    "from Environment.data_visualization import plot_graph, show_reset, show_done, show_step\n",
    "from Environment.episode_memory import Episode_Memory\n",
    "from Environment.createc_control import Createc_Controller\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the anchor image\n",
    "This cell retrieves the current STM scan image and use it as the template for positioning the anchor in STM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeed to connect\n",
      "(128, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d3086c50d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAANn0lEQVR4nO3df6jd9X3H8edrGv9JHSamNmlMo4UwcYOu7pLaOSRjtWgQ0j+kxD+qyOBiUWih/UMq2L8G2/4oTC3NAhUVig601bCl66yUxf6hM8ZETbVr6gQvCU0Xs1jbosv23h/3a3e5PTf33s/53nNO0ucDDuf743O+77cf5ZXv+Z7v16SqkKTl+r1xNyDp7GR4SGpieEhqYnhIamJ4SGpieEhqcv4wH06yFvgH4DLgDeCzVXVywLg3gF8A/wOcrqqpYepKGr9hzzzuAp6uqi3A0936Qv68qv7Y4JDODcOGxw7goW75IeAzQx5P0lkiw9xhmuS/quqiOesnq2rNgHH/AZwECvj7qtp9hmNOA9MAq1ev/pMrrriiub9z3YkTJ8bdwsQ7fvz4uFuYaO+++y6nT59Oy2cXveaR5PvA+gG77l5GnWuq6miSS4CnkrxWVfsGDeyCZTfA1NRU7d+/fxllfrc8+OCD425h4t13333jbmGivfbaa82fXTQ8qupTC+1L8rMkG6rqWJINwMCYr6qj3fvxJN8BtgIDw0PS2WHYax57gFu75VuBJ+cPSLI6yYXvLwOfBl4Zsq6kMRs2PP4auC7JT4DrunWSfDjJ3m7Mh4AfJjkE/BvwT1X1z0PWlTRmQ93nUVUngL8YsP0osL1bfh342DB1JE0e7zCV1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUpJfwSHJ9kh8nOZLkrgH7k+Tebv9LSa7qo66k8Rk6PJKcB3wduAG4Erg5yZXzht0AbOle08A3hq0rabz6OPPYChypqter6j3gUWDHvDE7gIdr1rPARUk29FBb0pj0ER4bgTfnrM9025Y7RtJZpI/wyIBt1TBmdmAynWR/kv0///nPh25O0sroIzxmgE1z1i8FjjaMAaCqdlfVVFVNffCDH+yhPUkroY/weB7YkuTyJBcAO4E988bsAW7pfnW5GjhVVcd6qC1pTM4f9gBVdTrJncD3gPOAB6rqcJLbu/27gL3AduAI8CvgtmHrShqvocMDoKr2MhsQc7ftmrNcwB191JI0GbzDVFITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1KTXsIjyfVJfpzkSJK7BuzfluRUkoPd654+6koan/OHPUCS84CvA9cBM8DzSfZU1Y/mDX2mqm4ctp6kydDHmcdW4EhVvV5V7wGPAjt6OK6kCTb0mQewEXhzzvoM8IkB4z6Z5BBwFPhyVR0edLAk08A0wPr163nxxRd7aPHc9Nhjj427hYl34MCBcbdwzurjzCMDttW89QPA5qr6GHAf8MRCB6uq3VU1VVVTa9as6aE9SSuhj/CYATbNWb+U2bOL36iqt6vqnW55L7Aqyboeaksakz7C43lgS5LLk1wA7AT2zB2QZH2SdMtbu7oneqgtaUyGvuZRVaeT3Al8DzgPeKCqDie5vdu/C7gJ+HyS08CvgZ1VNf+rjaSzSB8XTN//KrJ33rZdc5bvB+7vo5akyeAdppKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6Smhgekpr0Eh5JHkhyPMkrC+xPknuTHEnyUpKr+qgraXz6OvN4ELj+DPtvALZ0r2ngGz3VlTQmvYRHVe0D3jrDkB3AwzXrWeCiJBv6qC1pPEZ1zWMj8Oac9Zlu229JMp1kf5L9J0+eHElzkpZvVOGRAdtq0MCq2l1VU1U1tWbNmhVuS1KrUYXHDLBpzvqlwNER1Za0AkYVHnuAW7pfXa4GTlXVsRHVlrQCzu/jIEkeAbYB65LMAF8FVgFU1S5gL7AdOAL8Critj7qSxqeX8KiqmxfZX8AdfdSSNBm8w1RSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1KTXsIjyQNJjid5ZYH925KcSnKwe93TR11J49PLX3QNPAjcDzx8hjHPVNWNPdWTNGa9nHlU1T7grT6OJens0NeZx1J8Mskh4Cjw5ao6PGhQkmlgGmDjxo1cfPHFI2zx7HLttdeOu4WJt2/fvnG3MNF++ctfNn92VBdMDwCbq+pjwH3AEwsNrKrdVTVVVVNr164dUXuSlmsk4VFVb1fVO93yXmBVknWjqC1pZYwkPJKsT5JueWtX98QoaktaGb1c80jyCLANWJdkBvgqsAqgqnYBNwGfT3Ia+DWws6qqj9qSxqOX8KiqmxfZfz+zP+VKOkd4h6mkJoaHpCaGh6QmhoekJoaHpCaGh6QmhoekJoaHpCaGh6QmhoekJoaHpCaGh6QmhoekJoaHpCaGh6QmhoekJoaHpCaGh6QmhoekJoaHpCaGh6QmhoekJoaHpCaGh6QmhoekJoaHpCaGh6QmQ4dHkk1JfpDk1SSHk3xhwJgkuTfJkSQvJblq2LqSxquPv+j6NPClqjqQ5ELghSRPVdWP5oy5AdjSvT4BfKN7l3SWGvrMo6qOVdWBbvkXwKvAxnnDdgAP16xngYuSbBi2tqTx6fWaR5LLgI8Dz83btRF4c876DL8dMJLOIr2FR5IPAI8DX6yqt+fvHvCRWuA400n2J9n/1ltv9dWepJ71Eh5JVjEbHN+qqm8PGDIDbJqzfilwdNCxqmp3VU1V1dTatWv7aE/SCujj15YA3wReraqvLTBsD3BL96vL1cCpqjo2bG1J49PHry3XAJ8DXk5ysNv2FeAjAFW1C9gLbAeOAL8CbuuhrqQxGjo8quqHDL6mMXdMAXcMW0vS5PAOU0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNhg6PJJuS/CDJq0kOJ/nCgDHbkpxKcrB73TNsXUnjdX4PxzgNfKmqDiS5EHghyVNV9aN5456pqht7qCdpAgx95lFVx6rqQLf8C+BVYOOwx5U02fo48/iNJJcBHweeG7D7k0kOAUeBL1fV4QWOMQ1Md6vvbt68+ZU+exzSOuA/x93EHPazuEnradL6+YPWD6aqeukgyQeAfwX+qqq+PW/f7wP/W1XvJNkO/F1VbVnCMfdX1VQvDfbAfs5s0vqByevpXOqnl19bkqwCHge+NT84AKrq7ap6p1veC6xKsq6P2pLGo49fWwJ8E3i1qr62wJj13TiSbO3qnhi2tqTx6eOaxzXA54CXkxzstn0F+AhAVe0CbgI+n+Q08GtgZy3t+9LuHvrrk/2c2aT1A5PX0znTT2/XPCT9bvEOU0lNDA9JTSYmPJKsTfJUkp9072sWGPdGkpe729z3r0Af1yf5cZIjSe4asD9J7u32v5Tkqr57aOhpZLf/J3kgyfEkA++/GdP8LNbTSB+PWOIjGyObpxV7hKSqJuIF/C1wV7d8F/A3C4x7A1i3Qj2cB/wU+ChwAXAIuHLemO3Ad4EAVwPPrfC8LKWnbcA/jujf07XAVcArC+wf6fwssaeRzU9XbwNwVbd8IfDv4/zvaIn9LHuOJubMA9gBPNQtPwR8Zgw9bAWOVNXrVfUe8GjX11w7gIdr1rPARUk2jLmnkamqfcBbZxgy6vlZSk8jVUt7ZGNk87TEfpZtksLjQ1V1DGb/YYFLFhhXwL8keaG7lb1PG4E356zP8NuTvJQxo+4Jutv/k3w3yR+uYD+LGfX8LNVY5ucMj2yMZZ6W8gjJUueo12dbFpPk+8D6AbvuXsZhrqmqo0kuAZ5K8lr3J08fMmDb/N+ylzKmT0updwDYXP9/+/8TwKK3/6+QUc/PUoxlfrpHNh4HvlhVb8/fPeAjKzpPi/Sz7Dka6ZlHVX2qqv5owOtJ4Gfvn7Z178cXOMbR7v048B1mT+v7MgNsmrN+KbMP8i13TJ8WrVeTdfv/qOdnUeOYn8Ue2WDE87QSj5BM0teWPcCt3fKtwJPzByRZndn/ZwhJVgOfBvp86vZ5YEuSy5NcAOzs+prf5y3d1fKrgVPvf91aIYv2lMm6/X/U87OoUc9PV+uMj2wwwnlaSj9Nc7SSV52XeUX4YuBp4Cfd+9pu+4eBvd3yR5n9teEQcBi4ewX62M7s1eifvn984Hbg9m45wNe7/S8DUyOYm8V6urObj0PAs8CfrmAvjwDHgP9m9k/Pv5yA+Vmsp5HNT1fvz5j9CvIScLB7bR/XPC2xn2XPkbenS2oySV9bJJ1FDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lN/g+1Rgiwim7unwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "createc_controller = Createc_Controller(None, None, None, None)\n",
    "img_forward = np.array(createc_controller.stm.scandata(1,4))\n",
    "#TODO\n",
    "#Set the pixel of the top-left corner, widht, and height of the anchor\n",
    "#If the anchor is not used, just set w and h to a small number like below\n",
    "top_left, w, h = (0,0), 3, 3\n",
    "template = img_forward[top_left[1]:top_left[1]+h, top_left[0]:top_left[0]+w]\n",
    "plt.imshow(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters and initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeed to connect\n",
      "Load cnn weight\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "step_nm = 0.4 #Set the radius of the manipulation start position relative the the atom start position\n",
    "goal_nm  = 2 #Set the radius of the manipulation end position relative the the atom start position\n",
    "max_mvolt = 15 #Set the maximum bias voltage in mV \n",
    "max_pcurrent_to_mvolt_ratio = 6E3 #Set the maximum conductance in pA/mV\n",
    "max_len = 5 #Set maximum episode length\n",
    "template_max_y = 3 #Set the maximum or minimum row number to search for anchor\n",
    "#Set the path to load CNN weight for the atom movement classifier\n",
    "CNN_weight_path = 'C:/LocalUserData/User-data/phys-asp-lab/auto_manipulation/training_Ag_retrain_2/_atom_move_detector_conv_2640.pth'\n",
    "current_jump  = 4 #Set the current jump gradient/ std(current) threshold required to take STM scan\n",
    "\n",
    "#Set STM scan parameters\n",
    "pixel = 128 \n",
    "im_size_nm = 7 #Image size in nm \n",
    "scan_mV = 500 #bias voltage\n",
    "x_nm, y_nm = createc_controller.get_offset_nm()\n",
    "offset_nm = np.array([x_nm, y_nm]) #Set offset to current offset value\n",
    "\n",
    "#Set manipulation parameters to pull atoms from image edge to center\n",
    "pull_back_mV = 5 #bias in mV\n",
    "pull_back_pA = 60000 #current in pA\n",
    "\n",
    "#Set manipulation limit [left, right, up, down] in nm\n",
    "manip_limit_nm = np.array([x_nm - 0.5*im_size_nm+0.25, x_nm + 0.5*im_size_nm-0.25, y_nm+0.25, y_nm+im_size_nm-0.25])\n",
    "\n",
    "env = RealExpEnv(step_nm, max_mvolt, max_pcurrent_to_mvolt_ratio, goal_nm, \n",
    "                 template, current_jump, im_size_nm, offset_nm, manip_limit_nm, pixel, \n",
    "                 template_max_y, scan_mV, max_len, \n",
    "                 CNN_weight_path, \n",
    "                 bottom=False, random_scan_rate = 0.8, pull_back_mV = pull_back_mV,\n",
    "                 pull_back_pA = pull_back_pA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13268837332725525\n",
      "tensor([0.1327], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "batch_size= 64 #Set minibatch size\n",
    "LEARNING_RATE = 0.0003 #Set learning rate\n",
    "\n",
    "#Set the action space range\n",
    "ACTION_SPACE = namedtuple('ACTION_SPACE', ['high', 'low'])\n",
    "action_space = ACTION_SPACE(high = torch.tensor([1,1,1,1,1,1]), low = torch.tensor([-1,-1,-1,-1,1/3,1/2]))\n",
    "\n",
    "#Initialize the soft actor-critic agent\n",
    "agent = sac_agent(num_inputs = 4, num_actions = 6, action_space = action_space, device=device, hidden_size=256, lr=LEARNING_RATE,\n",
    "                 gamma=0.9, tau=0.005, alpha=alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Hindsight Experience Replay memory buffer\n",
    "Here we use the hindsight experience replay with the 'future' strategy to sample goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "replay_size=1000000 #Set memory size\n",
    "\n",
    "memory = HerReplayMemory(replay_size, env, strategy = 'future')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Episode Memory class\n",
    "The episode memory class is used to store all the relavant information in each training episode, including the STM images, state, action, reward, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_memory = Episode_Memory()\n",
    "#TODO\n",
    "#Set the folder name to store training data and neural network weight\n",
    "folder_name = 'C:/LocalUserData/User-data/phys-asp-lab/auto_manipulation/training_Ag_retrain_2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters for Emphasize Recent Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_k_min = 500\n",
    "eta = 0.994\n",
    "max_ep_len = max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty lists for logging performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards, alphas, precisions, episode_lengths = [], [], [], []\n",
    "avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_train(max_steps = max_len, num_episodes = 50, episode_start = 0):\n",
    "    \"\"\"\n",
    "    Collect training data and train the RL agent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_steps: int\n",
    "            maximum steps in an episode\n",
    "            \n",
    "    num_episodes: int\n",
    "            Train for this many episodes\n",
    "    \n",
    "    episode_start: int\n",
    "            Index to use for the starting episode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : None\n",
    "    \"\"\"\n",
    "    for i_episode in range(episode_start,episode_start+num_episodes):\n",
    "        print('Episode:', i_episode)\n",
    "        episode_reward, episode_steps = 0, 0\n",
    "        done = False\n",
    "        state, info = env.reset(update_conv_net=False)\n",
    "        show_reset(env.img_info, env.atom_start_absolute_nm, env.destination_absolute_nm,\n",
    "                   env.template_nm, env.template_wh)\n",
    "        episode_memory.update_memory_reset(env.img_info, i_episode, info)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            old_atom_nm = env.atom_absolute_nm\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_steps+=1\n",
    "            episode_reward+=reward\n",
    "            mask = float(not done)\n",
    "            memory.push(state,action,reward,next_state,mask)\n",
    "            episode_memory.update_memory_step(state, action, next_state, reward, done, info)\n",
    "            show_step(env.img_info, info['start_nm']+old_atom_nm, info['end_nm']+old_atom_nm,\n",
    "                        env.atom_absolute_nm, env.atom_start_absolute_nm, \n",
    "                        env.destination_absolute_nm, action[4]*env.max_mvolt, \n",
    "                        action[5]*env.max_pcurrent_to_mvolt_ratio*action[4]*env.max_mvolt, \n",
    "                        env.template_nm, env.template_wh)\n",
    "            print('step:', step,'reward', reward, 'precision:', env.dist_destination)\n",
    "            if done:\n",
    "                episode_memory.update_memory_done(env.img_info, env.atom_absolute_nm, env.atom_relative_nm)\n",
    "                episode_memory.save_memory(folder_name)\n",
    "                print('Episode reward:', episode_reward)\n",
    "                break\n",
    "            else:                \n",
    "                state=next_state\n",
    "             \n",
    "        if (len(memory)>batch_size):\n",
    "            episode_K = int(episode_steps)\n",
    "            for k in range(episode_K):\n",
    "                c_k = max(int(memory.__len__()*eta**((k)*(1000/episode_K))), 500)\n",
    "                agent.update_parameters(memory, batch_size, c_k)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        alphas.append(agent.alpha.item())\n",
    "        precisions.append(env.dist_destination)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        avg_episode_rewards.append(np.mean(episode_rewards[-min(100,len(episode_rewards)):]))\n",
    "        avg_alphas.append(np.mean(alphas[-min(100, len(alphas)):]))\n",
    "        avg_precisions.append(np.mean(precisions[-min(100, len(precisions)):]))\n",
    "        avg_episode_lengths.append(np.mean(episode_lengths[-min(100, len(episode_lengths)):]))\n",
    "        \n",
    "        if (i_episode+1)%2==0:\n",
    "            plot_graph(episode_rewards, precisions, alphas, episode_lengths,\n",
    "                      avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths)\n",
    "            \n",
    "        if (i_episode)%20 == 0:\n",
    "            torch.save(agent.critic.state_dict(), '{}/_critic_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.policy.state_dict(), '{}/_policy_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.alpha, '{}/_alpha_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(env.atom_move_detector.conv.state_dict(), '{}/_atom_move_detector_conv_{}.pth'.format(folder_name,i_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sac_train(episode_start = 2896,num_episodes = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
